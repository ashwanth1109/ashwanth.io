---
title: "Microservices architecture with AWS AppSync"
date: 2021-01-13
slug: "/microservices-architecture-with-appsync"
tags:
- AppSync
- DynamoDB
- GraphQL
- AWS
- Experiment
- Java
- Infra-As-Code
---

`youtube: https://www.youtube.com/watch?v=s0-BqmMnTQE`

As a part of the next experiment, I was interested in exploring an architecture that was built using AppSync.

The goal was to provide a secure, unified graphQL API for internal and external use, using AppSync.
External represents either the client application that accesses the API, or the API itself being exposed as a public API.
Internal (in this case) represents, any private API that is accessed purely by the development team, either for running scripts or integration tests.
There could be a lot of common APIs required between external and internal, but usually internal has a few additional requests that can be made based on the requirement.

## Architectural Options Considered

I considered two options for the design of such a system.

1. The straight-forward approach is to create an API with a schema that defines all your entities, connect it to a database (in our case, DynamoDB).
So, you have one API with one schema which contains all your types.

![Option 1: Default Approach](./Option1-Default-Approach.png)

2.Modern applications are created with microservices in mind, to promote rapid iteration because of smaller, faster deployments and because of improved fault tolerance (other modules are largely unaffected by the failure of one module).
The second approach involves splitting a single large graphQL API into micro APIs, each with its own schema and database.

![Option 2: Proposed Architecture](./Option2-Proposed-Architecture.png)

## Implementation

You can find the implementation code for this on this [repository here](https://github.com/ashwanth1109/appsync-microservices-outcome).

The implementation details the second approach -
To split a large AppSync API into micro APIs as proposed in architecture above.

In this experiment, we have one service (the Store service) and two microservices (Users and Orders).

The "Users" microservice has only one main entity in its schema:

```graphql
type User {
    id: ID!
    name: String
    orders: [ID]
}
```

Similarly, the "Orders" microservice has one main entity in its schema:

```graphql
type Order {
    id: ID!
    userId: String
    product: String
    quantity: Int
}
```

Both microservices have their own AppSync GraphQL API with all CRUD functionality related to their respective entity and related resolver templates.
You can deploy each individual stack and go to the "Queries" console on AppSync to perform all of these operations.

The last part of this puzzle is the Store service, which has 2 http Data Sources that connect to the two microservices.
The schema is composed of both "User" and "Order" type (a partial combination of both schemas), and you can define what operations are allowed on the Store API via this schema.

## Key things to note

#### 1. API Access Authorization

The CloudFormation stack sets up a cognito user pool authorization for the store service API.
The store service has a HTTP resolver that hits the graphql API endpoint of the orders microservice by forwarding the authorization token in the vtl resolver.
Currently, the orders microservices API also allows a user from the cognito pool to access the internal microservice API which might not be desirable.
In such a case, you can have the Store Service to take on an IAM role that allows access directly to the microservice API.

#### 2. Inbuilt Security Layer

If you look at the store schema, you will see that it does not have a deleteOrder mutation.
So, right now, only an internal call to the orders microservice can perform a delete operation.
Since the orders API endpoint is not publicly shared, a client does not have access to this operation.
This may circumvent the need to verify a users permission level at the VTL layer, but we can also add this check if needed for additional security.

#### 3. One microservice - One dynamoDB table

Each microservice corresponds to one single entity which should each have its own dynamoDB table.
There are several advantages with following this principle:

- You can look at the WCU and RCU consumption for each entity separately and analyze the tables individually to understand if there are any performance issues related to that specific microserviceâ€™s interaction with the database.
- If you have a need to run Scan or Query operations on the dynamoDB table (for example, if you want to fetch allOrders that belong to User 1), you will only be scanning through items related to the order entity and not any other entity.
- If you have any global indexes, the items that are projected into this index table will only be for the one entity and not any other items
- By grouping each micro API and its database into its own module, it becomes much easier to test and scale these microservices.

#### 4. Smaller, faster deployments

Since each microservice its own deployment cycle, you end up with smaller and faster deployments.

#### 5. Run tests only for service

When you want to merge a PR to a microservice, you only need to run tests related to that service API.

#### 6. Code generation tooling

There are a lot of instances of duplicate code across the 3 repos (store, orders, users). This is especially the case with deployment code, graphql schema files, resolvers etc.
We need to have good code generation tooling to make it easier to add more microservices as we go along.

#### 7. Orchestration complexity

We need to be wary of any orchestration complexity that is added when following this approach.
Inter-dependencies lead to a need to orchestrate steps in a certain manner.
For example, in this repo, you need to deploy `Store -> Users/Orders -> Store` in this order to test the APIs.


